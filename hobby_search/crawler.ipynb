{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.tools import *\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import math\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "tstamp = '%04d%02d%02d%02d%02d%02d' % (date.year, date.month ,date.day,date.hour,date.minute,date.second)\n",
    "\n",
    "ROOT_URL = \"https://www.1999.co.jp\"\n",
    "URL = \"https://www.1999.co.jp/search?typ1_c=101&cat=&state=&sold=0&sortid=0&searchkey=%E8%81%96%E9%97%98%E5%A3%AB%E6%98%9F%E7%9F%A2&spage=\"\n",
    "#URL= \"https://www.1999.co.jp/search?typ1_c=101&cat=figure&state=&sold=0&sortid=0&searchkey=%e8%81%96%e9%97%98%e5%a3%ab%e6%98%9f%e7%9f%a2\"\n",
    "OUT_F = 'data/figure_items_%s.csv' % tstamp\n",
    "LOG_F  = r\".\\log\\log%s.log\" % tstamp\n",
    "res = req.urlopen(URL)\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "NUM_PER_PAGE = 40\n",
    "SLEEP_SEC =1\n",
    "\n",
    "# Request URL & get html\n",
    "def get_soup(url, sleep_sec):\n",
    "\n",
    "    try:\n",
    "        # Not Local \n",
    "        res = req.urlopen(url)\n",
    "        soup = BeautifulSoup(res, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except urllib.error.HTTPError as http_e:\n",
    "        print( http_e.code , http_e.reason, url)\n",
    "        log_str = str( http_e.code) + http_e.reason +  url\n",
    "        write_log(LOG_F, str(http_e.code)  )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        sleep(sleep_sec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    try:\n",
    "        name = soup.find('h2',class_='h2_itemDetail').text\n",
    "    except:\n",
    "        name =''\n",
    "    finally:\n",
    "        return name\n",
    "    \n",
    "def get_price(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPrice\")\n",
    "        #print(soup)\n",
    "        price = soup.find('span',class_=\"Price_Dai\").text\n",
    "        if '¥' in price:\n",
    "            price = price.replace(\"\\n\",\"\")\n",
    "            price = price.replace(\"\\r\",\"\")\n",
    "            price = price.replace(\" \",\"\")\n",
    "            price = price.split('(税')[0]\n",
    "            price = price.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price = ''\n",
    "    finally:\n",
    "        return price\n",
    "\n",
    "def get_price_normal(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPriceNormal\")\n",
    "        for td_soup in soup.findAll('td'):\n",
    "                price_normal = td_soup.text\n",
    "                if '¥' in price_normal:\n",
    "                    price_normal = price_normal.split('(税')[0]\n",
    "                    price_normal = price_normal.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price_normal = ''\n",
    "    finally:\n",
    "        return price_normal\n",
    "\n",
    "\n",
    "def get_jan_code(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trJanCode\")\n",
    "        for jtest2 in soup.findAll('td'):\n",
    "            jan_code = jtest2.text\n",
    "            if jan_code.isdecimal():\n",
    "                return jan_code\n",
    "    except:\n",
    "        jan_code = \"\"\n",
    "    finally:\n",
    "        return jan_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maker(soup):\n",
    "    maker = soup.find(\"a\").text\n",
    "    return maker\n",
    "\n",
    "def get_details(soup):\n",
    "    item_details_soup = soup.find(\"table\", id=\"tblItemInfo\")\n",
    "    maker = scale = material = prototype = series = original =\"\"\n",
    "    for td in item_details_soup.findAll(\"tr\" ): \n",
    "        attri = td.text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "        try:\n",
    "            if \"メーカー：\" in attri:\n",
    "                maker = attri.replace(\"メーカー：\",\"\")\n",
    "            elif \"スケール：\" in attri:\n",
    "                scale = attri.replace(\"スケール：\",\"\")\n",
    "            elif \"素材：\" in attri:\n",
    "                material = attri.replace(\"素材：\",\"\")\n",
    "            elif \"原型制作：\" in attri:\n",
    "                prototype = attri.replace(\"原型制作：\",\"\")\n",
    "                print(\"test\")\n",
    "            elif \"シリーズ：\" in attri:\n",
    "                 series = attri.replace(\"シリーズ：\",\"\")\n",
    "            elif \"原作：\" in attri:\n",
    "                original  = attri.replace(\"原作：\",\"\")\n",
    "            elif \"発売\" in attri and \"日\" in attri:\n",
    "                 release_date = attri.split(\"：\")[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    details  = [maker, scale, material, prototype, series, original, release_date]\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(URL, SLEEP_SEC)\n",
    "\n",
    "searched_num = soup.find('div',class_='list_kensu02').text.split(\" \")[0]\n",
    "searched_product_num = int(searched_num)\n",
    "searched_page_num = math.ceil(searched_product_num / NUM_PER_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,searched_page_num+1):\n",
    "    print(\"page No = %s\" % i)\n",
    "    page_url = URL   + str(i)\n",
    "    page_soup = get_soup(page_url, SLEEP_SEC)\n",
    "    for produ_nm in page_soup.findAll('div','a',class_='ListItemName'):\n",
    "        produ_id = produ_nm.find('a').get(\"href\")\n",
    "        item_url = ROOT_URL + produ_id\n",
    "        item_soup = get_soup(ROOT_URL + produ_id,SLEEP_SEC)\n",
    "        name  = get_name(item_soup)\n",
    "        price = get_price(item_soup) \n",
    "        price_normal = get_price_normal(item_soup) \n",
    "        jan_code = get_jan_code(item_soup)\n",
    "        maker, scale, material, prototype, series, original, release_date = get_details(item_soup)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        item_info = [name, price,price_normal, jan_code, item_url, maker, scale, material, prototype, series, original, release_date]\n",
    "        write_info(OUT_F,item_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
