{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.tools import *\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import math\n",
    "import csv\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "tstamp = '%04d%02d%02d%02d%02d%02d' % (date.year, date.month ,date.day,date.hour,date.minute,date.second)\n",
    "\n",
    "ROOT_URL = \"https://www.1999.co.jp\"\n",
    "URL = \"https://www.1999.co.jp/search?typ1_c=101&cat=&state=&sold=0&sortid=0&searchkey=%E8%81%96%E9%97%98%E5%A3%AB%E6%98%9F%E7%9F%A2&spage=\"\n",
    "#URL= \"https://www.1999.co.jp/search?typ1_c=101&cat=figure&state=&sold=0&sortid=0&searchkey=%e8%81%96%e9%97%98%e5%a3%ab%e6%98%9f%e7%9f%a2\"\n",
    "OUT_F = 'data/figure_items_%s.csv' % tstamp\n",
    "LOG_F  = r\".\\log\\log%s.log\" % tstamp\n",
    "res = req.urlopen(URL)\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "NUM_PER_PAGE = 40\n",
    "SLEEP_SEC =1\n",
    "\n",
    "# Request URL & get html\n",
    "def get_soup(url, sleep_sec):\n",
    "\n",
    "    try:\n",
    "        # Not Local \n",
    "        res = req.urlopen(url)\n",
    "        soup = BeautifulSoup(res, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except urllib.error.HTTPError as http_e:\n",
    "        print( http_e.code , http_e.reason, url)\n",
    "        log_str = str( http_e.code) + http_e.reason +  url\n",
    "        write_log(LOG_F, str(http_e.code)  )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        sleep(sleep_sec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    try:\n",
    "        name = soup.find('h2',class_='h2_itemDetail').text\n",
    "    except:\n",
    "        name =''\n",
    "    finally:\n",
    "        return name\n",
    "    \n",
    "def get_price(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPrice\")\n",
    "        #print(soup)\n",
    "        price = soup.find('span',class_=\"Price_Dai\").text\n",
    "        if '¥' in price:\n",
    "            price = price.replace(\"\\n\",\"\")\n",
    "            price = price.replace(\"\\r\",\"\")\n",
    "            price = price.replace(\" \",\"\")\n",
    "            price = price.split('(税')[0]\n",
    "            price = price.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price = ''\n",
    "    finally:\n",
    "        return price\n",
    "\n",
    "def get_price_normal(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPriceNormal\")\n",
    "        for td_soup in soup.findAll('td'):\n",
    "                price_normal = td_soup.text\n",
    "                if '¥' in price_normal:\n",
    "                    price_normal = price_normal.split('(税')[0]\n",
    "                    price_normal = price_normal.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price_normal = ''\n",
    "    finally:\n",
    "        return price_normal\n",
    "\n",
    "\n",
    "def get_jan_code(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trJanCode\")\n",
    "        for jtest2 in soup.findAll('td'):\n",
    "            jan_code = jtest2.text\n",
    "            if jan_code.isdecimal():\n",
    "                return jan_code\n",
    "    except:\n",
    "        jan_code = \"\"\n",
    "    finally:\n",
    "        return jan_code\n",
    "\n",
    "def get_hight(soup):\n",
    "    hight =\"\"\n",
    "    try:\n",
    "        for div in item_soup.find(\"div\", id=\"masterBody_pnlItemExp\").findAll(\"div\"):\n",
    "            if  \"●全高：\" in div.text:\n",
    "                hight = div.text.split(\"●全高：\")[1].split(\"\\n\")[0].replace(\"\\n\",\"\").replace(\"\\r\",\"\")[:20]\n",
    "    except:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return hight\n",
    "\n",
    "def get_size(soup):\n",
    "    size =\"\"\n",
    "    try:\n",
    "        for div  in soup.find(\"div\", class_=\"txt14px\").findAll(\"div\",class_=\"\"):\n",
    "            size_str = div.text.replace(\"\\r\\n\",\"\")\n",
    "            if \"●サイズ：\" in size_str:\n",
    "                size = size_str.split(\"●サイズ：\")[1]\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "            return size\n",
    "\n",
    "        \n",
    "def get_img(img_show_url, out_dir):\n",
    "    img_url = get_soup(img_show_url,SLEEP_SEC).find(\"table\", id=\"imgTarget\").find(\"img\")[\"src\"]\n",
    "    \n",
    "    output_img_path = img_url.replace(\"/\",\"_\")\n",
    "    img_url = ROOT_URL + img_url\n",
    "    r = requests.get(img_url)\n",
    "    if os.path.exists(out_dir) == False:\n",
    "            os.mkdir(out_dir)\n",
    "    with open( out_dir + output_img_path,'wb') as file: \n",
    "        file.write(r.content)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maker(soup):\n",
    "    maker = soup.find(\"a\").text\n",
    "    return maker\n",
    "\n",
    "def get_details(soup):\n",
    "    item_details_soup = soup.find(\"table\", id=\"tblItemInfo\")\n",
    "    maker = scale = material = prototype = series = original = release_date = \"\"\n",
    "    for td in item_details_soup.findAll(\"tr\" ): \n",
    "        attri = td.text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "        try:\n",
    "            if \"メーカー：\" in attri:\n",
    "                maker = attri.replace(\"メーカー：\",\"\")\n",
    "            elif \"スケール：\" in attri:\n",
    "                scale = attri.replace(\"スケール：\",\"\")\n",
    "            elif \"素材：\" in attri:\n",
    "                material = attri.replace(\"素材：\",\"\")\n",
    "            elif \"原型制作：\" in attri:\n",
    "                prototype = attri.replace(\"原型制作：\",\"\")\n",
    "                print(\"test\")\n",
    "            elif \"シリーズ：\" in attri:\n",
    "                 series = attri.replace(\"シリーズ：\",\"\")\n",
    "            elif \"原作：\" in attri:\n",
    "                original  = attri.replace(\"原作：\",\"\")\n",
    "            elif \"発売\" in attri and \"日\" in attri:\n",
    "                 release_date = attri.split(\"：\")[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    details  = [maker, scale, material, prototype, series, original, release_date]\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(URL, SLEEP_SEC)\n",
    "\n",
    "searched_num = soup.find('div',class_='list_kensu02').text.split(\" \")[0]\n",
    "searched_product_num = int(searched_num)\n",
    "searched_page_num = math.ceil(searched_product_num / NUM_PER_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page No = 1\n",
      "page No = 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d5c65c64ee38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprodu_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprodu_nm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mitem_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROOT_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprodu_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mitem_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprodu_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSLEEP_SEC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mname\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-412b3450d641>\u001b[0m in \u001b[0;36mget_soup\u001b[0;34m(url, sleep_sec)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1,searched_page_num+1):\n",
    "    print(\"page No = %s\" % i)\n",
    "    page_url = URL   + str(i)\n",
    "    page_soup = get_soup(page_url, SLEEP_SEC)\n",
    "    \n",
    "    for produ_nm in page_soup.findAll('div','a',class_='ListItemName'):\n",
    "        produ_id = produ_nm.find('a').get(\"href\")\n",
    "        item_url = ROOT_URL + produ_id\n",
    "        item_soup = get_soup(ROOT_URL + produ_id,SLEEP_SEC)\n",
    "        name  = get_name(item_soup)\n",
    "        price = get_price(item_soup) \n",
    "        price_normal = get_price_normal(item_soup) \n",
    "        jan_code = get_jan_code(item_soup)\n",
    "        maker, scale, material, prototype, series, original, release_date = get_details(item_soup)\n",
    "        hight = get_hight(item_soup)\n",
    "        size = get_size(item_soup)\n",
    "        item_info = [produ_id,name, price,price_normal, jan_code, item_url, maker, scale, material, prototype, series, original, release_date,hight,size]\n",
    "        write_info(OUT_F,item_info)\n",
    "        img_list_soup = item_soup.find(\"div\", id=\"ImageDetail\")\n",
    "        try:\n",
    "            for url in img_list_soup.findAll('a'):    \n",
    "                img_show_url = url.get(\"href\" )\n",
    "                get_img(ROOT_URL + img_show_url,\"./data/images/%s/\" % produ_id)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
