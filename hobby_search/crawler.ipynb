{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.tools import *\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import math\n",
    "import csv\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now()\n",
    "tstamp = '%04d%02d%02d%02d%02d%02d' % (date.year, date.month ,date.day,date.hour,date.minute,date.second)\n",
    "\n",
    "ROOT_URL = \"https://www.1999.co.jp\"\n",
    "URL = \"https://www.1999.co.jp/search?typ1_c=101&cat=&state=&sold=0&sortid=0&searchkey=%E8%81%96%E9%97%98%E5%A3%AB%E6%98%9F%E7%9F%A2&spage=\"\n",
    "#URL= \"https://www.1999.co.jp/search?typ1_c=101&cat=figure&state=&sold=0&sortid=0&searchkey=%e8%81%96%e9%97%98%e5%a3%ab%e6%98%9f%e7%9f%a2\"\n",
    "OUT_F = 'data/figure_items_%s.csv' % tstamp\n",
    "LOG_F  = r\".\\log\\log%s.log\" % tstamp\n",
    "res = req.urlopen(URL)\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "NUM_PER_PAGE = 40\n",
    "SLEEP_SEC =1\n",
    "\n",
    "# Request URL & get html\n",
    "def get_soup(url, sleep_sec):\n",
    "\n",
    "    try:\n",
    "        # Not Local \n",
    "        res = req.urlopen(url)\n",
    "        soup = BeautifulSoup(res, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except urllib.error.HTTPError as http_e:\n",
    "        print( http_e.code , http_e.reason, url)\n",
    "        log_str = str( http_e.code) + http_e.reason +  url\n",
    "        write_log(LOG_F, str(http_e.code)  )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        sleep(sleep_sec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    try:\n",
    "        name = soup.find('h2',class_='h2_itemDetail').text\n",
    "    except:\n",
    "        name =''\n",
    "    finally:\n",
    "        return name\n",
    "    \n",
    "def get_price(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPrice\")\n",
    "        #print(soup)\n",
    "        price = soup.find('span',class_=\"Price_Dai\").text\n",
    "        if '¥' in price:\n",
    "            price = price.replace(\"\\n\",\"\")\n",
    "            price = price.replace(\"\\r\",\"\")\n",
    "            price = price.replace(\" \",\"\")\n",
    "            price = price.split('(税')[0]\n",
    "            price = price.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price = ''\n",
    "    finally:\n",
    "        return price\n",
    "\n",
    "def get_price_normal(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trPriceNormal\")\n",
    "        for td_soup in soup.findAll('td'):\n",
    "                price_normal = td_soup.text\n",
    "                if '¥' in price_normal:\n",
    "                    price_normal = price_normal.split('(税')[0]\n",
    "                    price_normal = price_normal.replace(\"¥\",\"\").replace(\",\",\"\")                    \n",
    "    except:\n",
    "        price_normal = ''\n",
    "    finally:\n",
    "        return price_normal\n",
    "\n",
    "\n",
    "def get_jan_code(soup):\n",
    "    try:\n",
    "        soup = soup.find('tr',id=\"masterBody_trJanCode\")\n",
    "        for jtest2 in soup.findAll('td'):\n",
    "            jan_code = jtest2.text\n",
    "            if jan_code.isdecimal():\n",
    "                return jan_code\n",
    "    except:\n",
    "        jan_code = \"\"\n",
    "    finally:\n",
    "        return jan_code\n",
    "\n",
    "def get_hight(soup):\n",
    "    try:\n",
    "        for div in item_soup.find(\"div\", id=\"masterBody_pnlItemExp\").findAll(\"div\"):\n",
    "            if  \"●全高：\" in div.text:\n",
    "                hight = div.text.split(\"●全高：\")[1].split(\"\\n\")[0].replace(\"\\n\",\"\").replace(\"\\r\",\"\")[:20]\n",
    "    except:\n",
    "        print(e)\n",
    "        hight = \"\"\n",
    "    finally:\n",
    "        return hight\n",
    "\n",
    "def get_size(soup):\n",
    "    size =\"\"\n",
    "    try:\n",
    "        for div  in soup.find(\"div\", class_=\"txt14px\").findAll(\"div\",class_=\"\"):\n",
    "            size_str = div.text.replace(\"\\r\\n\",\"\")\n",
    "            if \"●サイズ：\" in size_str:\n",
    "                size = size_str.split(\"●サイズ：\")[1]\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "            return size\n",
    "\n",
    "        \n",
    "def get_img(img_show_url, out_dir):\n",
    "    img_url = get_soup(img_show_url,SLEEP_SEC).find(\"table\", id=\"imgTarget\").find(\"img\")[\"src\"]\n",
    "    \n",
    "    print(img_url)\n",
    "    output_img_path = img_url.replace(\"/\",\"_\")\n",
    "    img_url = ROOT_URL + img_url\n",
    "    r = requests.get(img_url)\n",
    "    if os.path.exists(out_dir) == False:\n",
    "            os.mkdir(out_dir)\n",
    "    with open( out_dir + output_img_path,'wb') as file: \n",
    "        file.write(r.content)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maker(soup):\n",
    "    maker = soup.find(\"a\").text\n",
    "    return maker\n",
    "\n",
    "def get_details(soup):\n",
    "    item_details_soup = soup.find(\"table\", id=\"tblItemInfo\")\n",
    "    maker = scale = material = prototype = series = original = rellease_date = \"\"\n",
    "    for td in item_details_soup.findAll(\"tr\" ): \n",
    "        attri = td.text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "        try:\n",
    "            if \"メーカー：\" in attri:\n",
    "                maker = attri.replace(\"メーカー：\",\"\")\n",
    "            elif \"スケール：\" in attri:\n",
    "                scale = attri.replace(\"スケール：\",\"\")\n",
    "            elif \"素材：\" in attri:\n",
    "                material = attri.replace(\"素材：\",\"\")\n",
    "            elif \"原型制作：\" in attri:\n",
    "                prototype = attri.replace(\"原型制作：\",\"\")\n",
    "                print(\"test\")\n",
    "            elif \"シリーズ：\" in attri:\n",
    "                 series = attri.replace(\"シリーズ：\",\"\")\n",
    "            elif \"原作：\" in attri:\n",
    "                original  = attri.replace(\"原作：\",\"\")\n",
    "            elif \"発売\" in attri and \"日\" in attri:\n",
    "                 release_date = attri.split(\"：\")[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    details  = [maker, scale, material, prototype, series, original, release_date]\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(URL, SLEEP_SEC)\n",
    "\n",
    "searched_num = soup.find('div',class_='list_kensu02').text.split(\" \")[0]\n",
    "searched_product_num = int(searched_num)\n",
    "searched_page_num = math.ceil(searched_product_num / NUM_PER_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,searched_page_num+1):\n",
    "    print(\"page No = %s\" % i)\n",
    "    page_url = URL   + str(i)\n",
    "    page_soup = get_soup(page_url, SLEEP_SEC)\n",
    "    \n",
    "    for produ_nm in page_soup.findAll('div','a',class_='ListItemName'):\n",
    "        produ_id = produ_nm.find('a').get(\"href\")\n",
    "        item_url = ROOT_URL + produ_id\n",
    "        item_soup = get_soup(ROOT_URL + produ_id,SLEEP_SEC)\n",
    "        name  = get_name(item_soup)\n",
    "        price = get_price(item_soup) \n",
    "        price_normal = get_price_normal(item_soup) \n",
    "        jan_code = get_jan_code(item_soup)\n",
    "        maker, scale, material, prototype, series, original, release_date = get_details(item_soup)\n",
    "        hight = get_hight(item_soup)\n",
    "        size = get_size(item_soup)\n",
    "        item_info = [produ_id,name, price,price_normal, jan_code, item_url, maker, scale, material, prototype, series, original, release_date,hight,size]\n",
    "        write_info(OUT_F,item_info)\n",
    "        img_list_soup = item_soup.find(\"div\", id=\"ImageDetail\")\n",
    "        for url in img_list_soup.findAll('a'):    \n",
    "            img_show_url = url.get(\"href\" )\n",
    "            get_img(ROOT_URL + img_show_url,\"./data/images/%s/\" % produ_id)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_URL = \"https://www.1999.co.jp/10679272\" \n",
    "item_soup = get_soup(ITEM_URL, 1)\n",
    "img_list_soup = item_soup.find(\"div\", id=\"ImageDetail\")\n",
    "for url in img_list_soup.findAll('a'):    \n",
    "    img_show_url = url.get(\"href\" )\n",
    "    get_img(ROOT_URL + img_show_url,\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%E8%81%96%E9%97%98%E5%A3%AB%E6%98%9F%E7%9F%A2'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#soup = get_soup(URL,SLEEP_SEC)\n",
    "r = requests.get(URL)\n",
    "with open('./data/test.jpg','wb') as file: \n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_img(ROOT_URL  + \"/image/10679272/20/1\",\"./data/image_10679272_20_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_URL = \"https://www.1999.co.jp/10679272\"\n",
    "ITEM_IMG_URL =\"https://www.1999.co.jp/image/10679272/20/1\"\n",
    "img_soup = get_soup(ITEM_IMG_URL, SLEEP_SEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_url = img_soup.find(\"div\", class_=\"TagCenter\").find(\"img\")[\"src\"]\n",
    "img_url = img_soup.find(\"table\", id=\"imgTarget\").find(\"img\")[\"src\"]\n",
    "img_url\n",
    "get_img(ROOT_URL + img_url, \"data/\" + img_url.replace(\"/\",\"_\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "約160mm\r\n",
      "約160mm\r\n"
     ]
    }
   ],
   "source": [
    "for div in item_soup.find(\"div\", id=\"masterBody_pnlItemExp\").findAll(\"div\"):\n",
    "    if  \"●全高：\" in div.text:\n",
    "        hight = div.text.split(\"●全高：\")[1].split(\"\\n\")[0]\n",
    "        print(hight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    " s = item_soup.find(\"div\", id=\"masterBody_pnlItemExp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
